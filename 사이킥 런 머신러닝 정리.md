# 학습/테스트 데이터 세트 분리 : train_test_split()

train_test_split() 의 파라미터
test_size : 전체 데이터 세트에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인지 결정, 기본값 : 0.25
train_size : 전체 데이터 세트에서 학습용 데이터 세트 크기를 얼마로 샘플링 할 것인지 결정, test_size를 주로 사용해서 잘 쓰는 파라미터는 아님
shuffle : 데이터를 분리하기 전에 데이터를 미리 섞을지 결정, 기본값은 True, 데이터를 분산시켜 보다 효율적인 학습/테스트 데이터 세트를 만드는데 사용
random_state : 난수 값을 지정하면 여러번 다시 수행해도 동일한 결과가 나오게 해줌

# 2. 교차 검증(Cross Validation)

(1) K-Fold 교차검증(K-Fold Cross Validation)
K개의 데이터 폴드 세트를 만들어서 K번만큼 각 폴드 세트에 학습과 검증평가를 반복적으로 수행하는 방법

(2) Stratified KFold¶
불균형한(imbalanced) 분포도를 가진 레이블 데이터 집합을 위한 KFold 방식

--> 레이블 분포를 먼저 고려한 뒤 이 분포와 동일하게 학습과 검증 데이터 세트를 분배함

(3) cross_val_score() : 교차 검증을 보다 간단히게
앞서 수행한 Kfold로 데이터를 학습하고, 예측하는 코드를 보면 아래와 같은 프로세스로 검증이 진행되었음

(1) 폴드 세트 설정 
(2) for 루프 반복으로 학습 및 테스트 데이터의 인덱스를 추출 (3) 반복적으로 학습과 예측을 수행하고 예측수행을 반환

cross_val_score() 는 이 과정을 한꺼번에 수행해주는 API
cross_val_score() API의 선언형태
--> cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose =0, fit_params=None, pre_dispatch = '2*n_jobs')

estimator : 예측모델
X : 피처 데이터 세트
y : 레이블 데이터 세트,
scoring : 예측성능평가 지표
cv : 교차검증 폴드 수 (estimator로 classifier가 입력되면 Stratified Kfold 방식으로 학습/테스트 데이터 세트 분할)
수행 후 scoring 파라미터 값으로 지정된 성능 지표를 배열 형태로 반환

cross_val_score() API는 내부에서 estimator를 학습, 예측, 평가시켜주므로 간단히 교차검증을 수행할 수 있음
이와 유사하게 cross_validate() 은 여러개의 평가지표를 반환하며, 성능 평가지표와 수행시간도 같이 제공

(4) GridSearchCV - 교차검증과 하이퍼파라미터 튜닝을 한번에

GridSearch의 주요 파라미터
estimator : 적용 알고리즘 모델로 classifier, regressor, pipeline 등이 사용
param_grid : ket+리스트 값을 가지는 딕셔너리가 주어짐. estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지어
scoring : 예측 성능을 평가할 평가 방법을 지정, 문자열로 사이킷런의 성능평가 지표를 입력하나 별도의 함수 지정도 가능
refit : 기본값은 True이며 True로 생성시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 estimator 객체를 해당 하이퍼 파리미터로 재학습시킴

# 1. 데이터 인코딩
(1). 레이블 인코딩(Label Encoding)
 : 카테고리 피처를 코드형 숫자로 변환

LabelEncoder 클래스로 구현: 객체 생성 후 fit(), transform()

classes_ : 변환된 인코딩 값에 대한 원본 값을 가지고 있음

inverse_transform() : 인코딩된 값을 다시 디코딩할 수 있음

단점 : 일괄적인 숫자 값으로 변환이 되면서 몇몇 알고리즘에는 예측성능이 떨어지는 경우가 발생. (숫자 값의 크고 작음에 대한 특성이 반영되기 때문)

--> 회귀 알고리즘에는 적용하면 안됨, 트리 계열의 알고리즘은 숫자의 이런 특성을 반영하지 않으므로 레이블 인코딩도 별 문제가 없음

(2) 원-핫 인코딩(One-Hot Encoding)
바로 위에서 이야기한 레이블 인코딩의 문제점을 해결하기 위한 방식으로 OneHotEncoder 클래스로 쉽게 변환
피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지에는 0으로 표시하는 방법
주의점
OneHotEncoder 변환하기 전에 모든 문자열 값이 숫자형 값으로 변환이 되어 있어야 함
입력값으로 2차원 데이터가 필요함

get_dummies() : 판다스에서 이용할 수 있는 원핫 인코딩 API
사이킷런의 OneHotEncoder와는 달리 문자열 카테고리 값을 숫자형으로 변환할 필요 없이 바로 변환됨

# 2. 피처 스케일링과 정규화
피처 스케일링(Feature Scaling) : 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업(표준화, 정규화 등)

표준화(Standardization) : 데이터 피처의 각각이 평균이 0이고 분산이 1인 가우시안 정규분포를 가진 값으로 변환

정규화(Normalization) : 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념 (즉, 개별 데이터 크기를 모두 똑같은 단위로 변경)

1) StandardScaler
    : 표준화를 쉽게 지원하기 위한 클래스로 개별 피처를 평균이 0, 분산이 1인 값으로 변환
RBF 커널을 이용하는 서포트 벡터 머신, 선형회귀, 로지스틱 회귀는 데이터가 가우시안 분포를 가지고 있다고 가정하고 구현했기 때문에, 사전에 표준화를 적용하는 것은 예측 향상에 중요한 요소

(2) MinMaxScaler
    : 데이터값을 0과 1사이의 범위 값으로 변환(음수값이 있으면 -1에서 1값으로 변환)


# 1. 정확도(Accuracy) : 실제데이터와 예측데이터가 얼마나 같은지를 판단하는 지표
정확도 = 예측 데이터가 동일한 데이터 건수 / 전체 예측 데이터 건수
직관적으로 모델 예측 성능을 나타내는 평가 지표이지만 이진 분류의 경우 데이터의 구성에 따라 모델 성능을 왜곡할 수 있음

가령, 타이타닉 예제에서도 여성의 생존률이 높았기 때문에, 특별한 알고리즘 없이 여성을 생존, 남성을 사망으로 분류해도 정확도는 높을 수 있음 ( 단순히 하나의 조건만 가지고 결정하는 알고리즘도 높은 정확도를 나타내는 상황이 발생)

사이킷런의 BaseEstimator 클래스를 활용하여, 단순히 성별에 따라 생존자를 예측하는 단순한 분류기를 생성

2. 오차행렬/혼동행렬 (Confusion Matrix)
: 분류 문제에서 예측 오류가 얼마인지, 어떤 유형의 오류가 발생하고 있는지를 함께 나타내는 지표

TN(TrueNegative)  : 실제 값이 Negative인데 예측 값도 Negative
FP(FalsePositive) : 실제 값이 Negative인데 예측 값을 Positive
FN(FalseNegative) : 실제 값이 Positive인데 예측 값을 Negative
TP(TruePositive) : 실제 값이 Positive인데 예측 값도 Positive

3. 오차행렬을 통해 알 수 있는 지표들
Accuracy(정확도)=TN+TPTN+FP+FN+TP  

→ 예측결과와 실제값이 동일한 건수 / 전체 데이터 수
Precision(정밀도)=TPFP+TP  

→ 예측대상(Positive)을 정확히 예측한 수 / Positive로 예측한 데이터 수
Recall(재현율),Sensitivity(민감도),TruePositiveRate(TPR)=TPFN+TP  

→ Positive를 정확히 예측한 수 / 전체 Positive 데이터 수

pecificity(특이성),TrueNegativeRate(TNR)=TNTN+FP  

→ Negative를 정확히 예측한 수 / 전체 Negative 데이터 수
업무 특성에 따라서 특정지표가 유용하게 사용

ex) Recall(재현율) : 암 판정, 사기 판정 / Precision(정밀도) : 스팸메일 분류

4. 정밀도/재현율 트레이드오프
정밀도와 재현율은 상호보완적인 지표로 한쪽을 높이려고 하다보면 다른 한쪽이 떨어지기 쉬움
사이킷런의 분류 알고리즘은 예측 데이터가 특정 레이블에 속하는지 판단하기 위해 개별 레이블별로 확률을 구하고, 그 확률이 큰 레이블 값으로 예측
일반적으로는 임계값을 50%로 정하고 이보다 크면 Positive, 작으면 Negative로 결정
predict_proba( ) 를 통하여 개별 레이블별 예측확률을 반환받을 수 있음

precision_recall_curve(실제 클래스 값, 예측 확률 값) : 임계값 변화에 따른 평가 지표 값을 반환하는 API
반환 값 : 정밀도 - 임계값별 정밀도 값을 배열으로 반환, 재현율 - 임계값별 재현율 값을 배열으로 반환

4. F1 스코어
정밀도와 재현율을 결합한 지표로 정밀도와 재현율이 어느 한 쪽으로 치우치지 않을 때 상대적으로 높은 값을 가짐

F1=21recall+1precision =2∗precision∗recallprecision+recall 
 
예시 ) 모델 A : 정밀도 0.9, 재현율이 0.1 , 모델 B : 정밀도 0.5, 재현율 0.5 일 때

→ 모델 A의 F1 = 0.18 , 모델 B의 F1 = 0.5

f1_score(실제값, 예측값) : 사이킷런에서 F1스코어를 측정

5. ROC곡선과 AUC
ROC곡선  : FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선

Recall(재현율),Sensitivity(민감도),TPR=TPFN+TP  

→ 실제값 Positive가 정확이 예측되야 하는 수준(질병 보유자를 질병을 보유한 것으로 판정)
Specificity(특이성),TrueNegativeRate(TNR)=TNTN+FP  

→ 실제값 Negative가 정확히 예측되야 하는 수준(건강한 사람을 건강하다고 판정)
FPR=1−Specificity=FPTN+FP   --> 실제값 Negative 중 Positive로 잘못 예측된 비율(건강한 사람을 질병이 있다고 판정)





# DecisionTreeClassifier

_class_ sklearn.tree.DecisionTreeClassifier(_*_,  _criterion='gini'_,  _splitter='best'_,  _max_depth=None_,  _min_samples_split=2_,  _min_samples_leaf=1_,  _min_weight_fraction_leaf=0.0_,  _max_features=None_,  _random_state=None_,  _max_leaf_nodes=None_,  _min_impurity_decrease=0.0_,  _class_weight=None_,  _ccp_alpha=0.0_)

A decision tree classifier.

Decision Tree(결정트리) :
데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 알고리즘입니다.
(조금 더 쉽게 하자면 if else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘입니다.)

Decision Tree의 구조
루트노드(Root Node) : 시작점
리프노드(Leaf Node) : 결정된 클래스 값
규칙노드/내부노드(Decision Node / Internal Node) : 데이터세트의 피처가 결합해 만들어진 분류를 위한 규칙조건

※ 지니계수 : 경제학에서 불평등지수를 나타낼 때 사용하는 것으로 0일 때 완전 평등, 1일 때 완전 불평등을 의미합니다.

머신러닝에서는 데이터가 다양한 값을 가질수록 평등하며 특정 값으로 쏠릴 때 불평등한 값이 됩니다.
즉, 다양성이 낮을수록 균일도가 높다는 의미로 1로 갈수록 균일도가 높아 지니계수가 높은 속성을 기준으로 분할

2. Decision Tree의 장단점
장점
쉽고 직관적입니다.
각 피처의 스케일링과 정규화 같은 전처리 작업의 영향도가 크지 않습니다.
단점
규칙을 추가하며 서브트리를 만들어 나갈수록 모델이 복잡해지고, 과적합에 빠지기 쉽습니다.
→ 트리의 크기를 사전에 제한하는 튜닝이 필요합니다.

Parameters


**max_depth**int, default=None

- 트리의 최대 깊이
- default = None
→ 완벽하게 클래스 값이 결정될 때 까지 분할
또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할
- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요

**min_samples_split**int or float, default=2

- 노드를 분할하기 위한 최소한의 샘플 데이터수 → 과적합을 제어하는데 사용
- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가

**min_samples_leaf**int or float, default=1


- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수
- min_samples_split과 함께 과적합 제어 용도
- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요


**max_features**int, float or {“auto”, “sqrt”, “log2”}, default=None

- 최적의 분할을 위해 고려할 최대 feature 개수
- Default = None → 데이터 세트의 모든 피처를 사용
- int형으로 지정 →피처 갯수 / float형으로 지정 →비중
- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정
- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정

**random_state**int, RandomState instance or None, default=None

추정기의 랜덤성을 제어합니다. splitter가 best로 설정된 경우에도 각 분할에서 형상은 항상 랜덤하게 순열됩니다. "max_features < n_features"일 때 알고리즘은 각 분할에서 "max_features"를 무작위로 선택한 후 그 중 가장 좋은 분할을 찾는다. 그러나 가장 잘 발견된 분할은 'max_solution=n_solution'이 있더라도 실행마다 다를 수 있습니다. 기준의 개선이 여러 분할에 대해 동일하고 한 분할을 무작위로 선택해야 하는 경우이다. 피팅 중에 결정론적 동작을 얻으려면 'random_state'를 정수로 고정해야 한다.

**max_leaf_nodes**int, default=None

리프노드의 최대 개수

* Feature Importance
학습을 통해 규칙을 정하는 데 있어 피처의 중요도를 DecisionTreeClassifier 객체의 featureimportances 속성으로 확인할 수 있습니다.
→기본적으로 ndarray형태로 값을 반환하며 피처 순서대로 값이 할당

# 앙상블(Ensemble) 학습

## 앙상블이란 여러 개의 알고리즘을 사용하여, 그 예측을 결함함으로써 보다 정확한 예측을 도출하는 기법을 말합니다.
집단지성이 힘을 발휘하는 것처럼 단일의 강한 알고리즘보다 복수의 약한 알고리즘이 더 뛰어날 수 있다는 생각에 기반을 두고 있습니다.

이미지, 영상, 음성 등의 비정형 데이터의 분류는 딥러닝이 뛰어난 성능을 보이지만,
대부분 정형 데이터의 분류에서는 앙상블이 뛰어난 성능을 보이고 있다고 합니다.

앙상블 학습의 유형은 보팅(Voting), 배깅(Bagging), 부스팅(Boosting), 스태킹(Stacking) 등이 있습니다.

* 보팅은 여러 종류의 알고리즘을 사용한 각각의 결과에 대해 투표를 통해 최종 결과를 예측하는 방식입니다.
* 배깅은 같은 알고리즘에 대해 데이터 샘플을 다르게 두고 학습을 수행해 보팅을 수행하는 방식입니다.
이 때의 데이터 샘플은 중첩이 허용됩니다. 즉 10000개의 데이터에 대해 10개의 알고리즘이 배깅을 사용할 때,
각 1000개의 데이터 내에는 중복된 데이터가 존재할 수 있습니다.
* 배깅의 대표적인 방식이 Random Forest 입니다.

* 부스팅은 여러 개의 알고리즘이 순차적으로 학습을 하되, 앞에 학습한 알고리즘 예측이 틀린 데이터에 대해
올바르게 예측할 수 있도록, 그 다음번 알고리즘에 가중치를 부여하여 학습과 예측을 진행하는 방식입니다.

* 마지막으로 스태킹은 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어 다른 모델(메타 모델)로
재학습시켜 결과를 예측하는 방법입니다.

## 2. 하드보팅(Hard Voting)과 소프트보팅(Soft Voting)
하드보팅을 이용한 분류는 다수결 원칙과 비슷합니다.
소프트 보팅은 각 알고리즘이 레이블 값 결정 확률을 예측해서, 이것을 평균하여 이들 중 확률이 가장 높은 레이블 값을 최종 값으로 예측합니다.

일반적으로는 소프트 보팅이 성능이 더 좋아서 많이 적용됩니다.

## 3. 보팅 분류기(Voting Classifier)
사이킷런은 보팅방식의 앙상블을 구현한 VotingClassifier 클래스를 제공하고 있습니다.

사이킷런에서 제공되는 위스콘신 유방암 데이터 세트를 이용해 보팅방식의 앙상블을 적용해보겠습니다.



# RandomForestClassifier

2. 랜덤포레스트(RandomForest)
랜덤 포레스트는 여러 개의 결정트리(Decision Tree)를 활용한 배깅 방식의 대표적인 알고리즘

장점
결정 트리의 쉽고 직관적인 장점을 그대로 가지고 있음
앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있음
다양한 분야에서 좋은 성능을 나타냄
단점
하이퍼 파라미터가 많아 튜닝을 위힌 시간이 많이 소요됨

_class_ sklearn.ensemble.RandomForestClassifier(_n_estimators=100_,  _*_,  _criterion='gini'_,  _max_depth=None_,  _min_samples_split=2_,  _min_samples_leaf=1_,  _min_weight_fraction_leaf=0.0_,  _max_features='auto'_,  _max_leaf_nodes=None_,  _min_impurity_decrease=0.0_,  _bootstrap=True_,  _oob_score=False_,  _n_jobs=None_,  _random_state=None_,  _verbose=0_,  _warm_start=False_,  _class_weight=None_,  _ccp_alpha=0.0_,  _max_samples=None_)

1. 배깅(Bagging)이란?
배깅(Bagging)은 Bootstrap Aggregating의 약자로, 보팅(Voting)과는 달리 동일한 알고리즘으로 여러 분류기를 만들어 보팅으로 최종 결정하는 알고리즘

**배깅은 다음과 같은 방식으로 진행이 됩니다.

(1) 동일한 알고리즘을 사용하는 일정 수의 분류기 생성
(2)각각의 분류기는 부트스트래핑(Bootstrapping)방식으로 생성된 샘플데이터를 학습
(3)최종적으로 모든 분류기가 보팅을 통헤 예측 결정

※ 부트스트래핑 샘플링은 전체 데이터에서 일부 데이터의 중첩을 허용하는 방식

랜덤포레스트 하이퍼 파라미터 튜닝
랜덤포레스트는 트리기반의 하이퍼 파라미터에 배깅, 부스팅, 학습, 정규화 등을 위한 하이퍼 파라미터까지 추가되므로 튜닝할 파라미터가 많습니다.

Parameters

**n_estimators**int, default=100

- 결정트리의 갯수를 지정
- Default = 10
- 무작정 트리 갯수를 늘리면 성능 좋아지는 것 대비 시간이 걸릴 수 있음

**max_depth**int, default=None

- 트리의 최대 깊이
- default = None
→ 완벽하게 클래스 값이 결정될 때 까지 분할
또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할
- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요

**min_samples_split**int or float, default=2

- 노드를 분할하기 위한 최소한의 샘플 데이터수
→ 과적합을 제어하는데 사용
- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가
    
**min_samples_leaf**int or float, default=1

- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수
- min_samples_split과 함께 과적합 제어 용도
- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요

**max_features**{“auto”, “sqrt”, “log2”}, int or float, default=”auto”

- 최적의 분할을 위해 고려할 최대 feature 개수
- Default = 'auto' (결정트리에서는 default가 none이었음)
- int형으로 지정 →피처 갯수 / float형으로 지정 →비중
- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정
- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정

**max_leaf_nodes**int, default=None

리프노드의 최대 개수


# 1. Boosting Algorithm
부스팅 알고리즘은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치를 부여해 오류를 개선해나가며 학습하는 방식

부스팅 알고리즘은 대표적으로 아래와 같은 알고리즘들이 있음

AdaBoost
Gradient Booting Machine(GBM)
XGBoost
LightGBM
CatBoost

## 2. AdaBoost
Adaptive Boost의 줄임말로서 약한 학습기(weak learner)의 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘
속도나 성능적인 측면에서 decision tree를 약한 학습기로 사용함

AdaBoost의 하이퍼파라미터

파라미터 명	설명

base_estimators	- 학습에 사용하는 알고리즘
- Default = None
→ DecisionTreeClassifier(max_depth=1)가 적용
n_estimators	- 생성할 약한 학습기의 갯수를 지정
- Default = 50
learning_rate	- 학습을 진행할 때마다 적용하는 학습률(0~1)
- Weak learner가 순차적으로 오류 값을 보정해나갈 때 적용하는 계수
- Default = 1.0

## 3. Gradient Boost Machine(GBM)
GBM의 학습 방식
AdaBoost와 유사하지만, 가중치 업데이트를 경사하강법(Gradient Descent)를 이용하여 최적화된 결과를 얻는 알고리즘입니다.
GBM은 예측 성능이 높지만 Greedy Algorithm으로 과적합이 빠르게되고, 시간이 오래 걸린다는 단점이 있습니다.

※ 경사하강법
분류의 실제값을 y, 피처에 기반한 예측함수를 F(x), 오류식을 h(x) = y-F(x)라고 하면 이 오류식을 최소화하는 방향성을 가지고 가중치 값을 업데이트

GBM의 하이퍼파라미터
Tree에 관한 하이퍼 파라미터

파라미터 명	설명

max_depth	- 트리의 최대 깊이
- default = 3
- 깊이가 깊어지면 과적합될 수 있으므로 적절히 제어 필요
min_samples_split	- 노드를 분할하기 위한 최소한의 샘플 데이터수
→ 과적합을 제어하는데 사용
- Default = 2 → 작게 설정할 수록 분할 노드가 많아져 과적합 가능성 증가
min_samples_leaf	- 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수
- min_samples_split과 함께 과적합 제어 용도
- default = 1
- 불균형 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 필요
max_features	- 최적의 분할을 위해 고려할 최대 feature 개수
- Default = 'none' → 모든 피처 사용
- int형으로 지정 →피처 갯수 / float형으로 지정 →비중
- sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정
- log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정
max_leaf_nodes	- 리프노드의 최대 개수
- default = None → 제한없음

Boosting에 관한 하이퍼파라미터

파라미터 명	설명

loss	- 경사하강법에서 사용할 cost function 지정
- 특별한 이유가 없으면 default 값인 deviance 적용
n_estimators	- 생성할 트리의 갯수를 지정
- Default = 100
- 많을소록 성능은 좋아지지만 시간이 오래 걸림
learning_rate	- 학습을 진행할 때마다 적용하는 학습률(0~1)
- Weak learner가 순차적으로 오류 값을 보정해나갈 때 적용하는 계수
- Default = 0.1
- 낮은 만큼 최소 오류 값을 찾아 예측성능이 높아질 수 있음
- 하지만 많은 수의 트리가 필요하고 시간이 많이 소요
subsample	- 개별 트리가 학습에 사용하는 데이터 샘플링 비율(0~1)
- default=1 (전체 데이터 학습)
- 이 값을 조절하여 트리 간의 상관도를 줄일 수 있음

# XGBoost(eXtra Gradient Boost)

## 1. XGBoost(eXtra Gradient Boost)의 개요
트리 기반의 알고리즘의 앙상블 학습에서 각광받는 알고리즘 중 하나입니다.
GBM에 기반하고 있지만, GBM의 단점인 느린 수행시간, 과적합 규제 등을 해결한 알고리즘입니다.

XGBoost의 주요장점
(1) 뛰어난 예측 성능
(2) GBM 대비 빠른 수행 시간
(3) 과적합 규제(Overfitting Regularization)
(4) Tree pruning(트리 가지치기) : 긍정 이득이 없는 분할을 가지치기해서 분할 수를 줄임
(5) 자체 내장된 교차 검증

반복 수행시마다 내부적으로 교차검증을 수행해 최적회된 반복 수행횟수를 가질 수 있음
지정된 반복횟수가 아니라 교차검증을 통해 평가 데이트세트의 평가 값이 최적화되면 반복을 중간에 멈출 수 있는 기능이 있음
(6) 결손값 자체 처리

XGBoost는 독자적인 XGBoost 모듈과 사이킷런 프레임워크 기반의 모듈이 존재합니다.
독자적인 모듈은 고유의 API와 하이퍼파라미터를 사용하지만, 사이킷런 기반 모듈에서는 다른 Estimator와 동일한 사용법을 가지고 있습니다.

## 2. XGBoost의 하이퍼 파라미터

일반 파라미터
: 일반적으로 실행 시 스레드의 개수나 silent 모드 등의 선택을 위한 파라미터, default 값을 바꾸는 일은 거의 없음

파라미터 명	설명
booster	- gbtree(tree based model) 또는 gblinear(linear model) 중 선택
- Default = 'gbtree'
silent	- Default = 1
- 출력 메시지를 나타내고 싶지 않을 경우 1로 설정
nthread	- CPU 실행 스레드 개수 조정
- Default는 전체 다 사용하는 것
- 멀티코어/스레드 CPU 시스템에서 일부CPU만 사용할 때 변경

주요 부스터 파라미터
: 트리 최적화, 부스팅, regularization 등과 관련된 파라미터를 지칭

파라미터 명
(파이썬 래퍼)	파라미터명

eta(0.3)	learning rate(0.1)	- GBM의 learning rate와 같은 파라미터 - 범위: 0 ~ 1

num_boost_around(10)	n_estimators(100)	- 생성할 weak learner의 수

min_child_weight (1)	min_child_weight(1)	- GBM의 min_samples_leaf와 유사
- 관측치에 대한 가중치 합의 최소를 말하지만
GBM에서는 관측치 수에 대한 최소를 의미
- 과적합 조절 용도
- 범위: 0 ~ ∞

gamma(0)	min_split_loss(0)	- 리프노드의 추가분할을 결정할 최소손실 감소값
- 해당값보다 손실이 크게 감소할 때 분리
- 값이 클수록 과적합 감소효과
- 범위: 0 ~ ∞

max_depth(6)	max_depth(3)	- 트리 기반 알고리즘의 max_depth와 동일
- 0을 지정하면 깊이의 제한이 없음
- 너무 크면 과적합(통상 3~10정도 적용)
- 범위: 0 ~ ∞

sub_sample(1)	subsample(1)	- GBM의 subsample과 동일
- 데이터 샘플링 비율 지정(과적합 제어)
- 일반적으로 0.5~1 사이의 값을 사용
- 범위: 0 ~ 1

colsample_bytree(1)	colsample_bytree(1)	- GBM의 max_features와 유사
- 트리 생성에 필요한 피처의 샘플링에 사용
- 피처가 많을 때 과적합 조절에 사용
- 범위: 0 ~ 1

lambda(1)	reg_lambda(1)	- L2 Regularization 적용 값
- 피처 개수가 많을 때 적용을 검토
- 클수록 과적합 감소 효과

alpha(0)	reg_alpha(0)	- L1 Regularization 적용 값
- 피처 개수가 많을 때 적용을 검토
- 클수록 과적합 감소 효과

scale_pos_weight(1)	scale_pos_weight(1)	- 불균형 데이터셋의 균형을 유지


학습 태스크 파라미터
: 학습 수행 시의 객체함수, 평가를 위한 지표 등을 설정하는 파라미터

파라미터 명	설명
objective	- ‘reg:linear’ : 회귀
- binary:logistic : 이진분류
- multi:softmax : 다중분류, 클래스 반환
- multi:softprob : 다중분류, 확륣반환
eval_metric	- 검증에 사용되는 함수정의
- 회귀 분석인 경우 'rmse'를, 클래스 분류 문제인 경우 'error'
----------------------------------------------------
- rmse : Root Mean Squared Error
- mae : mean absolute error
- logloss : Negative log-likelihood
- error : binary classification error rate
- merror : multiclass classification error rate
- mlogloss: Multiclass logloss
- auc: Area Under Curve


과적합 제어
eta 값을 낮춥니다.(0.01 ~ 0.1) → eta 값을 낮추면 num_boost_round(n_estimator)를 반대로 높여주어야 합니다.
max_depth 값을 낮춥니다.
min_child_weight 값을 높입니다.
gamma 값을 높입니다.
subsample과 colsample_bytree를 낮춥니다.


※ Early Stopping 기능 :
GBM의 경우 n_estimators에 지정된 횟수만큼 학습을 끝까지 수행하지만, XGB의 경우 오류가 더 이상 개선되지 않으면 수행을 중지
n_estimators 를 200으로 설정하고, 조기 중단 파라미터 값을 50으로 설정하면, 1부터 200회까지 부스팅을 반복하다가
50회를 반복하는 동안 학습오류가 감소하지 않으면 더 이상 부스팅을 진행하지 않고 종료합니다.
(가령 100회에서 학습오류 값이 0.8인데 101~150회 반복하는 동안 예측 오류가 0.8보다 작은 값이 하나도 없으면 부스팅을 종료)


roc_curve(실제값, 예측 확률 값) : FPR, TPR, 임계값을 반환
데이터의 분포가 가우시안 분포가 아닐 경우에 적용해 볼 수 있음
